{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxhrIC/mvZ3Jy4oJmMQYWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okalenskyy/DS_Boilerplate/blob/main/01_DataAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.1 Detect anomalies in a time-series dataset."
      ],
      "metadata": {
        "id": "g7vmnDYcixhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Outlier Detection"
      ],
      "metadata": {
        "id": "Qj5jCzI4xCsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Z-Score\n",
        "\n",
        "The Z-score method is a statistically based approach for outlier detection.\n",
        "1. Compute the standard score, or Z-score, for each data point.\n",
        "2. Compute how many standard deviations a data point deviates from the mean of the dataset.\n",
        "3. Set a threshold for Z-score, and data points with Z-scores greater than it are considered outliers.\n",
        "An important assumption made by the Z-score method is that the data is **normally distributed**, making it especially useful for datasets with symmetrical patterns around the mean."
      ],
      "metadata": {
        "id": "hY97CixExWKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from scipy import stats\n",
        "\n",
        "threshold = 2.5 # Set threshold\n",
        "df = load_breast_cancer(as_frame=True).data # Assign dataframe\n",
        "z_scores = stats.zscore(df)\n",
        "outliers = df[abs(z_scores) > threshold]"
      ],
      "metadata": {
        "id": "SSZ0rdgnxDsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Pros***\n",
        "* Ease of implementation\n",
        "* Assumes that the data is distributed normally, which is a widely applicable assumption for situations in the real world.\n",
        "* Offers a numerical assessment of the extremeness of each outlier based on standard deviations.\n",
        "\n",
        "***Cons:***\n",
        "\n",
        "* If the data is not normally distributed, Z-score will not be effective for detecting outliers. **The distribution check must be done before the aaplication!**\n",
        "* It may be influenced by the presence of other outliers in the dataset.\n",
        "\n",
        "**Note:** Check the dataset and context for the **threshold value** selection - this must be done carefully. *This is the place for experiment.*\n"
      ],
      "metadata": {
        "id": "O9OoiEV0ynin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LOF - Local Outlier Factor\n",
        "\n",
        "The Local Outlier Factor algorithm calculates a data point’s local density deviation in relation to its neighbors. LOF assigns an anomaly score to each data point, indicating how likely it is to be an outlier. Outliers are points that have a high anomaly score.\n",
        "\n",
        "1. Calculate the LOF score for each data point by comparing the local density of each data point to the local densities of its neighbors.\n",
        "2. An outlier is a data point whose local density is significantly lower than that of its neighbors.\n",
        "\n",
        "LOF is useful for datasets with a range of densities or clusters, because it considers the concept of local density.\n",
        "\n",
        "In **scikit-learn**, we can convert LOF scores to predictions by using the predict or *fit_predict* method, which assigns a values:\n",
        "\n",
        "*    **1** - point is not an outlier\n",
        "*   **-1** - point is likely to be outlier"
      ],
      "metadata": {
        "id": "1o_DJbi4zYXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "data = load_breast_cancer(as_frame=True).data\n",
        "lof = LocalOutlierFactor(n_neighbours=20,contamination=0.1)\n",
        "outliers = lof.fit_predict(data)\n",
        "data[\"LOF\"] = outliers"
      ],
      "metadata": {
        "id": "raK33fddzOcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Pros:***\n",
        "\n",
        "* Effective in identifying outliers in datasets with varying densities or clusters.\n",
        "* Doesn’t require assumptions about the underlying distribution of the data.\n",
        "Provides anomaly scores that can be used to rank the outliers.\n",
        "\n",
        "***Cons:***\n",
        "\n",
        "* Sensitivity to the choice of parameters such as the number of neighbors (n_neighbors) and the contamination rate (contamination).\n",
        "* Can be computationally expensive for large datasets.\n",
        "* May require careful interpretation and adjustment of the anomaly scores threshold for outlier detection."
      ],
      "metadata": {
        "id": "-ivLZRoJ1t-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Isolation Forest\n",
        "\n",
        "The Isolation Forest algorithm is an effective and efficient **unsupervised ** outlier detection tool. It operates by isolating outliers as abnormalities in a random forest structure. Unlike typical decision trees, which divide data into non-overlapping sections, the Isolation Forest method **randomly selects features** and splits data points until outliers are isolated into individual leaves.\n",
        "\n",
        "The approach takes advantage of the fact that outliers are expected to have shorter average path lengths in the random forest, making them easier to isolate.\n",
        "\n",
        "1. Assign an anomaly score to each data point.    \n",
        "2. In scikit-learn, use the *predict* or *fit_predict* method.\n",
        "which assign a value:\n",
        "*  1 - data point unlikely to be outlier\n",
        "* -1 - point that is likely an outlier\n",
        "\n",
        "**Lower scores indicating a higher risk of being an outlier.**"
      ],
      "metadata": {
        "id": "wH3qpDlC2I0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "data = load_breast_cancer(as_frame=True).data\n",
        "iso = IsolationForest(contamination=0.1)\n",
        "outliers = iso.fit_predict(data)\n",
        "data[\"ISO\"] = outliers"
      ],
      "metadata": {
        "id": "dhpCJOtK3mFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Pros:***\n",
        "\n",
        "* Effective in identifying outliers **in high-dimensional** datasets.\n",
        "* Can handle datasets with **mixed variable types** (numeric and categorical).\n",
        "* Efficient for processing large datasets due to its random partitioning strategy.\n",
        "\n",
        "***Cons:***\n",
        "* Sensitivity to the choice of parameters, especially the contamination rate.\n",
        "* May require tuning of hyperparameters, such as the number of trees in the forest.\n",
        "* Interpretation of anomaly scores can be challenging!\n"
      ],
      "metadata": {
        "id": "AZg2SOoJ3wNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DBSCAN\n",
        "\n",
        "DBSCAN is a density-based clustering technique that can also detect outliers. It gathers data points that are close to each other depending on a distance criterion. **Outliers** are data points that a**re far removed from any cluster**.\n",
        "\n",
        "DBSCAN defines three types of data points:\n",
        "* **Core Points** Data points within a specified neighborhood of a minimum number of other data points.\n",
        "* **Border Points** Data points within the specified neighborhood of a core point, but do not have enough neighboring points to be considered core themselves.\n",
        "* **Noise Points (Outliers)** Data points that are neither core nor border points.\n",
        "\n",
        "Does not require a specification of the number of clusters.\n",
        "Identifies outliers based on their separation from dense data regions. The *fit_predict* method of the DBSCAN estimator fits the model to the data, and the labels_ attribute contains the cluster labels assigned to each data point. **Outliers** are identified as data points **labeled as -1**.\n"
      ],
      "metadata": {
        "id": "J3IIe5IR4laP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "data = load_breast_cancer(as_frame=True).data\n",
        "dbscan = DBSCAN()\n",
        "outliers = dbscan.fit_predict(data[['mean radius']])\n",
        "data[\"DBSCAN\"] = outliers"
      ],
      "metadata": {
        "id": "i8SinKUV7kY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pros:**\n",
        "\n",
        "* Doesn’t require specifying the number of clusters in advance.\n",
        "* Ideal for datasets with an unknown number of clusters\n",
        "* Effective in detecting outliers in datasets with irregular shapes and varying densities.\n",
        "* Robust to noise and able to handle datasets with complex structures.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "* Sensitivity to the choice of parameters, especially the eps and min_samples values.\n",
        "* Performance can degrade for high-dimensional datasets.\n",
        "* When use higher dimensions, will end up marking almost every point as an outlier.\n",
        "* Difficulty in determining optimal parameter values for different datasets."
      ],
      "metadata": {
        "id": "TB_bWFr58Kf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Conduct time-series analysis."
      ],
      "metadata": {
        "id": "DjTEMokSi609"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Create and analyze graph data using something like cuGraph.\n"
      ],
      "metadata": {
        "id": "Bidbkrdgi-BH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4 Identify how much data is big data (or when to use which acceleration method).\n"
      ],
      "metadata": {
        "id": "smDDNsQajCV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.5 Perform exploratory data analysis (EDA).\n"
      ],
      "metadata": {
        "id": "d2vAweNujDln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.6 Visualize time-series data.\n"
      ],
      "metadata": {
        "id": "sKsT2PbMjExW"
      }
    }
  ]
}